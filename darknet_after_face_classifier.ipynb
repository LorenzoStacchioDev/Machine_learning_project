{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from  keras.models import Sequential, Model\n",
    "from  keras.layers import Input, Dense, LeakyReLU, Activation, Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D, InputLayer, Flatten, BatchNormalization, Reshape, Lambda\n",
    "\n",
    "from  keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from  keras.applications.vgg16 import VGG16, preprocess_input\n",
    "#from  keras.applications import ResNet152V2,ResNet101V2,ResNet50V2 \n",
    "#from  keras.applications.resnet_v2 import preprocess_input \n",
    "\n",
    "from  keras.optimizers import RMSprop, Adam\n",
    "from  keras.preprocessing.image import load_img, ImageDataGenerator\n",
    "from  keras.utils import multi_gpu_model\n",
    "from IPython.display import Image \n",
    "\n",
    "import keras_metrics\n",
    "import pandas\n",
    "import ast\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image, ImageDraw,ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolf_loss(y_true, y_pred):\n",
    "    #true e pred sono [32][245]\n",
    "    \n",
    "    #tensorflow.print(\"tensors:\", y_pred)\n",
    "    #tensorflow.print(\"batch_size:\", tensorflow.shape(y_pred)[0])\n",
    "    \n",
    "    #y_pred = tensorflow.reshape(y_pred, [tensorflow.shape(y_pred)[0], 7, 7, 5])\n",
    "    #y_true = tensorflow.reshape(y_true, [tensorflow.shape(y_pred)[0], 7, 7, 5])\n",
    "\n",
    "\n",
    "    b_p_pred = y_pred[0]\n",
    "    b_x_pred = y_pred[1]\n",
    "    b_y_pred = y_pred[2]\n",
    "    b_w_pred = y_pred[3]\n",
    "    b_h_pred = y_pred[4]\n",
    "\n",
    "\n",
    "    b_p = y_true[0]\n",
    "    b_x = y_true[1]\n",
    "    b_y = y_true[2]\n",
    "    b_w = y_true[3]\n",
    "    b_h = y_true[4]\n",
    "\n",
    "    #print(b_xy_pred.get_shape(),b_xy.get_shape())\n",
    "    #print(b_wh_pred.get_shape(),b_wh.get_shape())\n",
    "    #indicator_coord = K.expand_dims(y_true[ 3], axis=-1) * 1.0\n",
    "    loss_p =K.sum(K.square(b_p - b_p_pred), axis=-1)\n",
    "    loss_xy = K.sum(b_p * (K.square(b_x - b_x_pred) + K.square(b_y - b_y_pred)), axis=-1)# * indicator_coord)#, axis=[1,2,3,4])\n",
    "    \n",
    "    b_w = K.pow(b_w, 0.5)\n",
    "    b_h = K.pow(b_h, 0.5)\n",
    "    b_w_pred = K.pow(b_w_pred, 0.5)\n",
    "    b_h_pred = K.pow(b_h_pred, 0.5)\n",
    "    \n",
    "    loss_wh = K.sum(\n",
    "        b_p * \n",
    "        (\n",
    "            (K.square(b_w - b_w_pred)) + \n",
    "            (K.square(b_h - b_h_pred))\n",
    "        ), axis=-1)# * indicator_coord)#, axis=[1,2,3,4])\n",
    "\n",
    "    #tensorflow.print(\"loss_p:\", loss_p)\n",
    "    #tensorflow.print(\"loss_xy:\", loss_xy)\n",
    "    #tensorflow.print(\"loss_wh:\", loss_wh)\n",
    "\n",
    "    #print(K.cast(loss_p, dtype=\"float32\"))\n",
    "    #print(K.cast(loss_xy, dtype=\"float32\"))\n",
    "    #print(loss_wh)\n",
    "    #tensorflow.print(\"Loss:\", ( K.cast(loss_p, dtype=\"float32\") + loss_wh +  K.cast(loss_xy, dtype=\"float32\") )/3)\n",
    "    return (K.cast(loss_p, dtype=\"float32\") + loss_wh +  K.cast(loss_xy, dtype=\"float32\"))/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pandas.read_csv(\"./dataset/FACE_CLASSIFIER/train2.csv\")\n",
    "valid_df = pandas.read_csv(\"./dataset/FACE_CLASSIFIER/val2.csv\")\n",
    "test_df = pandas.read_csv(\"./dataset/FACE_CLASSIFIER/test2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48859 validated image filenames.\n",
      "Found 5430 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    #rescale=1./255,\n",
    "    preprocessing_function = preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory=\"./\",\n",
    "        x_col=\"image_path\",\n",
    "        y_col=[\"p\",\"x\",\"y\",\"w\",\"h\"],\n",
    "        class_mode=\"raw\",\n",
    "        shuffle=True,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=valid_df,\n",
    "        directory=\"./\",\n",
    "        x_col=\"image_path\",\n",
    "        y_col=[\"p\",\"x\",\"y\",\"w\",\"h\"],\n",
    "        class_mode=\"raw\",\n",
    "        shuffle=True,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size_train = train_generator.samples/train_generator.batch_size\n",
    "step_size_valid = valid_generator.samples/valid_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknetv1 = Sequential()\n",
    "darknetv1.add(InputLayer(input_shape=(IMG_SIZE,IMG_SIZE,3)))\n",
    "darknetv1.add(Conv2D(64,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "darknetv1.add(Conv2D(192,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "darknetv1.add(Conv2D(128,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(256,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(256,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(512,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "darknetv1.add(Conv2D(256,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(512,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(256,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(512,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(256,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(512,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(256,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(512,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(512,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(1024,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "darknetv1.add(Conv2D(512,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(1024,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(512,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(1024,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(GlobalAveragePooling2D())\n",
    "\n",
    "darknetv1.add(Dense(512, activation = \"relu\"))\n",
    "darknetv1.add(Dense(512, activation = \"relu\"))\n",
    "darknetv1.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknetv1.load_weights(\"face_classifier_BN.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pandas.read_csv(\"./dataset/FACE_CLASSIFIER/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>face</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.\\dataset\\FACE_CLASSIFIER\\test.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.\\dataset\\FACE_CLASSIFIER\\test2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.\\dataset\\FACE_CLASSIFIER\\test3.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.\\dataset\\FACE_CLASSIFIER\\256_ObjectCategories...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.\\dataset\\FACE_CLASSIFIER\\256_ObjectCategories...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path  face\n",
       "0                 .\\dataset\\FACE_CLASSIFIER\\test.jpg     1\n",
       "1                .\\dataset\\FACE_CLASSIFIER\\test2.jpg     1\n",
       "2                .\\dataset\\FACE_CLASSIFIER\\test3.jpg     1\n",
       "3  .\\dataset\\FACE_CLASSIFIER\\256_ObjectCategories...     0\n",
       "4  .\\dataset\\FACE_CLASSIFIER\\256_ObjectCategories...     0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "            dataframe=test_df,\n",
    "            directory=\"./\",\n",
    "            x_col=\"image_path\",\n",
    "            y_col=\"face\",\n",
    "            class_mode=\"raw\",\n",
    "            shuffle=False,\n",
    "            target_size=(IMG_SIZE, IMG_SIZE),\n",
    "            batch_size=16)\n",
    "\n",
    "STEP_SIZE_TEST = test_generator.n / test_generator.batch_size \n",
    "\n",
    "\n",
    "#CHANGE PARALLEL MODEL\n",
    "pred=darknetv1.predict_generator(test_generator,  steps=STEP_SIZE_TEST,  verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img = Image.open(test_generator.filenames[0])\n",
    "img1 = ImageDraw.Draw(img)\n",
    "font = ImageFont.truetype(\"arial.ttf\", 50)\n",
    "# get a drawing context\n",
    "# draw text, half opacity\n",
    "img1.text((0,0), str(pred[0]), font = font,fill=(0,0,0,0))\n",
    "#img.save(\"./dataset/FACE_CLASSIFIER/testlabel3\" + \".jpg\")\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    darknetv1.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in darknetv1.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 224, 224, 64)      256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 224, 224, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 192)     110784    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 112, 112, 192)     768       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 112, 112, 192)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 128)       24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 56, 56, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 56, 56, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 56, 56, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 56, 56, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 256)       131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 256)       131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 28, 28, 256)       131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 28, 28, 256)       131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 28, 28, 512)       262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 28, 28, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 28, 28, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 28, 28, 1024)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 14, 14, 512)       524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 14, 14, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 14, 14, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 14, 14, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 14, 14, 512)       524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 14, 14, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 14, 14, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 14, 14, 1024)      0         \n",
      "=================================================================\n",
      "Total params: 22,433,856\n",
      "Trainable params: 0\n",
      "Non-trainable params: 22,433,856\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "darknetv1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknetv1.add(Conv2D(1024,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(1024,3, strides=(2,2), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(1024,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Conv2D(1024,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv1.add(BatchNormalization())\n",
    "darknetv1.add(LeakyReLU(alpha=0.1))\n",
    "darknetv1.add(Flatten())\n",
    "darknetv1.add(Dense(128, activation=\"relu\"))\n",
    "darknetv1.add(Dense(5, activation=\"relu\"))\n",
    "#darknetv1.add(Conv2D(5,1, strides=(1,1), padding = \"same\", activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 224, 224, 64)      256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 224, 224, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 192)     110784    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 112, 112, 192)     768       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 112, 112, 192)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 192)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 128)       24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 56, 56, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 56, 56, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 56, 56, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 56, 56, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 256)       131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 256)       131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 28, 28, 256)       131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 28, 28, 256)       131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 28, 28, 512)       262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 28, 28, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 28, 28, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 28, 28, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 28, 28, 1024)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 14, 14, 512)       524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 14, 14, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 14, 14, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 14, 14, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 14, 14, 512)       524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 14, 14, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 14, 14, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 14, 14, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 14, 14, 1024)      9438208   \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 14, 14, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 14, 14, 1024)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 7, 7, 1024)        9438208   \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 7, 7, 1024)        9438208   \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 7, 7, 1024)        9438208   \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 50176)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               6422656   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 66,626,373\n",
      "Trainable params: 44,184,325\n",
      "Non-trainable params: 22,442,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "darknetv1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknetv1.compile(optimizer=Adam(lr = 1e-4), loss=yolf_loss, metrics=[yolf_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "darknetv1.evaluate_generator(valid_generator, steps=step_size_valid, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "3054/3053 [==============================] - 463s 151ms/step - loss: 196274.6292 - yolf_loss: 196274.6094 - val_loss: 7736316.0000 - val_yolf_loss: 497213440.0000\n",
      "Epoch 2/15\n",
      "2428/3053 [======================>.......] - ETA: 1:27 - loss: 170444.2661 - yolf_loss: 170444.1250"
     ]
    }
   ],
   "source": [
    "#earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint('darknet_ev.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='min')\n",
    "\n",
    "history = darknetv1.fit_generator(generator=train_generator, epochs=15, steps_per_epoch=step_size_train, validation_data=valid_generator, validation_steps=step_size_valid, verbose=1, callbacks=[mcp_save, reduce_lr_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training accuracy')\n",
    "plt.plot(val_acc, label='Validation accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = darknetv1.to_json()\n",
    "with open(\"darknet_ev.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "#model.save_weights(\"resnet50.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknetv2 = Sequential()\n",
    "darknetv2.add(InputLayer(input_shape=(IMG_SIZE,IMG_SIZE,3)))\n",
    "darknetv2.add(Conv2D(64,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "darknetv2.add(Conv2D(192,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "darknetv2.add(Conv2D(128,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(256,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(256,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(512,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "darknetv2.add(Conv2D(256,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(512,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(256,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(512,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(256,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(512,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(256,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(512,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(512,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(1024,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "darknetv2.add(Conv2D(512,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(1024,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(512,1, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(1024,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(1024,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(1024,3, strides=(2,2), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(1024,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(1024,3, strides=(1,1), padding = \"same\"))\n",
    "darknetv2.add(BatchNormalization())\n",
    "darknetv2.add(LeakyReLU(alpha=0.1))\n",
    "darknetv2.add(Conv2D(5,1, strides=(1,1), padding = \"same\", activation=\"relu\"))\n",
    "darknetv2.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknetv2.load_weights(\"darknet_ev.h5\") \n",
    "darknetv2.compile(optimizer=Adam(lr = 1e-4), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=valid_df,\n",
    "        directory=validation_set,\n",
    "        x_col=\"image_id\",\n",
    "        y_col=train_df.columns[1:],\n",
    "        class_mode=\"raw\",\n",
    "        shuffle=True,\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE)\n",
    "STEP_SIZE_TEST = test_generator.n / test_generator.batch_size \n",
    "\n",
    "\n",
    "#CHANGE PARALLEL MODEL\n",
    "pred=darknetv2.predict_generator(test_generator,  steps=STEP_SIZE_TEST,  verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred[0].reshape(49,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_generator.labels[0].reshape(49,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "max_row = -1\n",
    "list_max = []\n",
    "filename= \"\"\n",
    "for el in zip(pred,test_generator.labels,test_generator.filenames):\n",
    "    count = count +1\n",
    "    for row in el[1].reshape(49,5):\n",
    "        if row[1] > max_row:\n",
    "            max_row =  row[1]\n",
    "            list_max = []\n",
    "            list_max.append(row)\n",
    "            filename = el[2]\n",
    "    try:\n",
    "        list_max = [item for sublist in list_max for item in sublist]\n",
    "        img = Image.open(validation_set+\"\\\\\"+filename)\n",
    "        img1 = ImageDraw.Draw(img)\n",
    "        x1,y1 = (list_max[1]-(list_max[3]/2)),(list_max[2]-(list_max[4]/2))\n",
    "        x4,y4= (list_max[1]+(list_max[3]/2)),(list_max[2]+(list_max[4]/2))\n",
    "        img1.rectangle([(x1,y1),(x4,y4)], outline =\"red\") \n",
    "        img.save(\"./dataset/NUOVO/results/output\"+ str(count) + \".jpg\")\n",
    "        #img.show() \n",
    "    except:\n",
    "        pass\n",
    "    if count == 1000:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
