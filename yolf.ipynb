{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation, Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D, InputLayer, Flatten, BatchNormalization\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.image import load_img, ImageDataGenerator\n",
    "\n",
    "import keras_metrics\n",
    "import pandas\n",
    "import ast\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = \".\\\\dataset\\\\NUOVO\\\\train\"\n",
    "validation_set = \".\\\\dataset\\\\NUOVO\\\\val\"\n",
    "test_set = \".\\\\dataset\\\\NUOVO\\\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pandas.read_csv(\".\\\\dataset\\\\NUOVO\\\\train\\\\output.csv\")\n",
    "valid_df = pandas.read_csv(\".\\\\dataset\\\\NUOVO\\\\val\\\\output.csv\")\n",
    "test_df = pandas.read_csv(\".\\\\dataset\\\\NUOVO\\\\test\\\\output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 164105 validated image filenames.\n",
      "Found 18234 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    brightness_range=[0.4,1.0])\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory='.\\\\dataset\\\\NUOVO\\\\train',\n",
    "        x_col=\"image_id\",\n",
    "        y_col=[\"x_1\",\"y_1\",\"width\",\"height\"],\n",
    "        class_mode=\"raw\",\n",
    "        shuffle=True,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=64)\n",
    "\n",
    "valid_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=valid_df,\n",
    "        directory='.\\\\dataset\\\\NUOVO\\\\val',\n",
    "        x_col=\"image_id\",\n",
    "        y_col=[\"x_1\",\"y_1\",\"width\",\"height\"],\n",
    "        class_mode=\"raw\",\n",
    "        shuffle=True,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=64)\n",
    "\n",
    "train_crops = train_generator\n",
    "val_crops = valid_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size_train = train_generator.samples/train_generator.batch_size\n",
    "step_size_valid = valid_generator.samples/valid_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\lorenzo\\desktop\\universit√†\\machine_learning_project\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(224,224,3)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64, kernel_size=(1,1), activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(1,1), activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(1,1), activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(1,1), activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "          \n",
    "model.add(Conv2D(filters=1024, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(1,1), activation=\"relu\"))\n",
    "model.add(Conv2D(filters=1024, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(1,1), activation=\"relu\"))\n",
    "model.add(Conv2D(filters=1024, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "\n",
    "model.add(Conv2D(filters=4, kernel_size=(1,1)))\n",
    "#model.add(GlobalAveragePooling2D())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 224, 224, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 128)       32896     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 14, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 256)       131328    \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 256)       131328    \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 14, 14, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 7, 7, 1024)        4719616   \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 7, 7, 512)         524800    \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 7, 7, 1024)        4719616   \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 7, 7, 512)         524800    \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 7, 7, 1024)        4719616   \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 7, 7, 4)           4100      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 7, 7, 4)           16        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               19700     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 19,848,464\n",
      "Trainable params: 19,846,472\n",
      "Non-trainable params: 1,992\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracking <tf.Variable 'Variable:0' shape=() dtype=int32> tp\n",
      "tracking <tf.Variable 'Variable_1:0' shape=() dtype=int32> fp\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(), loss='mse', metrics=[keras_metrics.precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\lorenzo\\desktop\\universit√†\\machine_learning_project\\venv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "285/284 [==============================] - 25s 89ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[56953.02734375, 0.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(val_crops, steps=step_size_valid, use_multiprocessing=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2565/2564 [==============================] - 490s 191ms/step - loss: 17544.0402 - precision: 55.5013 - val_loss: 21756.5117 - val_precision: 169.7817\n",
      "Epoch 2/5\n",
      "2565/2564 [==============================] - 486s 190ms/step - loss: 15835.2101 - precision: 52.3492 - val_loss: 5049.6841 - val_precision: 184.8901\n",
      "Epoch 3/5\n",
      "2565/2564 [==============================] - 487s 190ms/step - loss: 15245.4388 - precision: 54.5572 - val_loss: 19425.7617 - val_precision: 174.2422\n",
      "Epoch 4/5\n",
      "2565/2564 [==============================] - 493s 192ms/step - loss: 14645.5572 - precision: 53.9407 - val_loss: 8159.5503 - val_precision: 186.9954\n",
      "Epoch 5/5\n",
      "2565/2564 [==============================] - 486s 190ms/step - loss: 14135.6020 - precision: 54.0284 - val_loss: 13851.8926 - val_precision: 205.5939\n"
     ]
    }
   ],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=12, verbose=1, mode='min')\n",
    "mcp_save = ModelCheckpoint('yolf.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, mode='min')\n",
    "\n",
    "history = model.fit_generator(generator=train_crops, epochs=5, steps_per_epoch=step_size_train, validation_data=val_crops, validation_steps=step_size_valid, verbose=1, callbacks=[mcp_save, earlyStopping, reduce_lr_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['precision']\n",
    "val_acc = history.history['val_precision']\n",
    "\n",
    "rec = history.history['recall']\n",
    "val_rec = history.history['val_recall']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(acc, label='Training Precision')\n",
    "plt.plot(val_acc, label='Validation Precision')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([min(plt.ylim()),200])\n",
    "plt.title('Training and Validation Precision')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(rec, label='Training Recall')\n",
    "plt.plot(val_rec, label='Validation Recall')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Recall')\n",
    "plt.ylim([min(plt.ylim()),200])\n",
    "plt.title('Training and Validation Recall')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,10000])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(history.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
